{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomTheSheep/Unsloth/blob/master/FINAL_OCR_ONLY\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpuDTxo6xdVL"
      },
      "source": [
        "## OCR-Only Prompting with Metrics\n",
        "This notebook runs inference using only OCR text by providing a blank image to the multimodal model. It then calculates detailed performance metrics."
      ],
      "id": "kpuDTxo6xdVL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp8aKnxYxdVM"
      },
      "source": [
        "### 1. Installation"
      ],
      "id": "Jp8aKnxYxdVM"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f3c3foxQxdVM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ],
      "id": "f3c3foxQxdVM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dat0dfU8xdVN"
      },
      "source": [
        "### 2. Load Model and Tokenizer"
      ],
      "id": "Dat0dfU8xdVN"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NZOmfqC3xdVN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "2bef83de-a6fb-4715-fd4a-868abd5780e4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth currently only works on NVIDIA GPUs and Intel GPUs.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4066774320.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastVisionModel\u001b[0m \u001b[0;31m# FastLanguageModel for LLMs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m fourbit_models = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_device_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth currently only works on NVIDIA GPUs and Intel GPUs."
          ]
        }
      ],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ],
      "id": "NZOmfqC3xdVN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BohESIpTxdVN"
      },
      "source": [
        "### 3. Imports and Path Setup"
      ],
      "id": "BohESIpTxdVN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RaY7BVgxdVN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Set up Directories and Paths\n",
        "root_dir = \"/DocVQA\"\n",
        "ocr_dir = os.path.join(root_dir, \"OCR\", \"Val\")\n",
        "qa_dir = os.path.join(root_dir, \"QA\", \"Val\")\n",
        "\n",
        "# Output JSON file for results\n",
        "output_path = \"/content/prompt_outputs_ocr_only_val.json\"\n",
        "\n",
        "# Prepare model for inference\n",
        "FastVisionModel.for_inference(model)"
      ],
      "id": "_RaY7BVgxdVN"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def extract_text_from_ocr(ocr_data):\n",
        "    \"\"\"Extract all text from OCR JSON data.\"\"\"\n",
        "    text_content = []\n",
        "    if \"recognitionResults\" in ocr_data:\n",
        "        for result in ocr_data[\"recognitionResults\"]:\n",
        "            if \"lines\" in result:\n",
        "                for line in result[\"lines\"]:\n",
        "                    text_content.append(line[\"text\"])\n",
        "    return \"\\n\".join(text_content)\n",
        "\n",
        "def levenshtein_distance(s1, s2):\n",
        "    \"\"\"Calculates the Levenshtein distance between two strings.\"\"\"\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein_distance(s2, s1)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    s = s.lower()\n",
        "    s = \"\".join(ch for ch in s if ch not in set(string.punctuation))\n",
        "    s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
        "    s = \" \".join(s.split())\n",
        "    return s\n",
        "\n",
        "def calculate_anls(prediction, ground_truths):\n",
        "    \"\"\"\n",
        "    Calculates the Average Normalized Levenshtein Similarity (ANLS).\n",
        "    The final score is the maximum ANLS score against all ground truth answers.\n",
        "    \"\"\"\n",
        "    if not ground_truths:\n",
        "        return 0.0\n",
        "\n",
        "    prediction_norm = normalize_text(prediction)\n",
        "    max_anls = 0.0\n",
        "\n",
        "    for gt in ground_truths:\n",
        "        gt_norm = normalize_text(gt)\n",
        "        distance = levenshtein_distance(prediction_norm, gt_norm)\n",
        "        max_len = max(len(prediction_norm), len(gt_norm))\n",
        "        if max_len == 0:\n",
        "            anls = 1.0 if distance == 0 else 0.0\n",
        "        else:\n",
        "            anls = 1.0 - (distance / max_len)\n",
        "        if anls > max_anls:\n",
        "            max_anls = anls\n",
        "\n",
        "    return max_anls\n",
        "\n",
        "def calculate_extractive_match(prediction, ground_truths):\n",
        "    \"\"\"\n",
        "    Calculates if any ground truth is a substring of the prediction.\n",
        "    Returns 1.0 for a match, 0.0 otherwise.\n",
        "    \"\"\"\n",
        "    if not ground_truths:\n",
        "        return 0.0\n",
        "\n",
        "    prediction_norm = normalize_text(prediction)\n",
        "\n",
        "    for gt in ground_truths:\n",
        "        gt_norm = normalize_text(gt)\n",
        "        if gt_norm in prediction_norm:\n",
        "            return 1.0 # Found a match\n",
        "\n",
        "    return 0.0 # No match found\n",
        "\n",
        "def calculate_strict_match(prediction, ground_truths):\n",
        "    \"\"\"\n",
        "    Calculates the strict match category based on normalized text.\n",
        "    \"\"\"\n",
        "    prediction_norm = normalize_text(prediction)\n",
        "    has_prediction = bool(prediction_norm)\n",
        "    has_ground_truth = bool(ground_truths)\n",
        "\n",
        "    if has_ground_truth:\n",
        "        if not has_prediction:\n",
        "            return \"Missing\"  # Expected an answer, but got none.\n",
        "\n",
        "        ground_truths_norm = [normalize_text(gt) for gt in ground_truths]\n",
        "        if prediction_norm in ground_truths_norm:\n",
        "            return \"Correct\"  # Prediction matches a ground truth.\n",
        "        else:\n",
        "            return \"Incorrect\"  # Prediction does not match any ground truth.\n",
        "    else: # No ground truth\n",
        "        if has_prediction:\n",
        "            return \"Spurious\"  # Predicted an answer, but none was expected.\n",
        "        else:\n",
        "            return \"Correct\" # Correctly predicted nothing when nothing was expected.\n",
        "\n",
        "def calculate_confusion_matrix(prediction, ground_truths):\n",
        "    \"\"\"\n",
        "    Calculates TP, FP, TN, FN based on extractive match logic.\n",
        "    \"\"\"\n",
        "    prediction_norm = normalize_text(prediction)\n",
        "    has_prediction = bool(prediction_norm)\n",
        "    has_ground_truth = bool(ground_truths)\n",
        "\n",
        "    # Positive Case: A ground truth exists.\n",
        "    if has_ground_truth:\n",
        "        match_found = any(normalize_text(gt) in prediction_norm for gt in ground_truths)\n",
        "        if match_found:\n",
        "            return {\"tp\": 1, \"fp\": 0, \"tn\": 0, \"fn\": 0}  # True Positive\n",
        "        else:\n",
        "            return {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 1}  # False Negative\n",
        "    # Negative Case: No ground truth exists.\n",
        "    else:\n",
        "        if not has_prediction:\n",
        "            return {\"tp\": 0, \"fp\": 0, \"tn\": 1, \"fn\": 0}  # True Negative\n",
        "        else:\n",
        "            return {\"tp\": 0, \"fp\": 1, \"tn\": 0, \"fn\": 0}  # False Positive"
      ],
      "metadata": {
        "id": "gzRj612vyZ1s"
      },
      "id": "gzRj612vyZ1s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect all QA files\n",
        "qa_files = sorted(os.listdir(qa_dir))\n",
        "\n",
        "# Initialize the output data list\n",
        "all_data = []\n",
        "\n",
        "# Initialize counters\n",
        "strict_match_counts = {\"Correct\": 0, \"Incorrect\": 0, \"Missing\": 0, \"Spurious\": 0}\n",
        "confusion_matrix_counts = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
        "\n",
        "# Create a blank image to pass to the model\n",
        "blank_image = Image.new('RGB', (100, 100), 'black')\n",
        "\n",
        "# Iterate through each QA file\n",
        "for qa_filename in qa_files:\n",
        "    file_id = qa_filename.replace(\".json\", \"\")\n",
        "    qa_file_path = os.path.join(qa_dir, qa_filename)\n",
        "    ocr_file_path = os.path.join(ocr_dir, f\"{file_id}.json\")\n",
        "\n",
        "    if not os.path.exists(ocr_file_path):\n",
        "        print(f\"OCR file not found for {file_id}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Load and extract text from the OCR data\n",
        "    with open(ocr_file_path, \"r\") as f:\n",
        "        ocr_data = json.load(f)\n",
        "    document_text = extract_text_from_ocr(ocr_data)\n",
        "\n",
        "    # Load the questions for the current document\n",
        "    with open(qa_file_path, \"r\") as f:\n",
        "        qa_items = json.load(f)\n",
        "\n",
        "    for qa in qa_items:\n",
        "        question = qa[\"question\"]\n",
        "        ground_truth_answers = qa.get(\"answers\", [])\n",
        "\n",
        "        # Prepare the OCR-only prompt\n",
        "        enhanced_question = f\"\"\"Based on the following text, please answer the question.\n",
        "\n",
        "Document text:\n",
        "{document_text}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # Prepare the messages payload for the tokenizer\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\", \"text\": enhanced_question}\n",
        "            ]}\n",
        "        ]\n",
        "\n",
        "        # Tokenize the inputs (blank image + text)\n",
        "        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "        inputs = tokenizer(\n",
        "            blank_image,\n",
        "            input_text,\n",
        "            add_special_tokens=False,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        # Generate the response from the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=128,\n",
        "                use_cache=False,\n",
        "                temperature=1.5,\n",
        "                min_p=0.1,\n",
        "                output_scores=True,\n",
        "                return_dict_in_generate=True,\n",
        "            )\n",
        "\n",
        "        # Extract generated tokens and scores\n",
        "        generated_ids = outputs.sequences[:, inputs.input_ids.shape[1]:]\n",
        "        scores = outputs.scores\n",
        "\n",
        "        # Calculate confidence score\n",
        "        token_confidences = []\n",
        "        if scores:\n",
        "            for i, token_id in enumerate(generated_ids[0]):\n",
        "                logits = scores[i][0]\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                confidence = probs[token_id].item()\n",
        "                token_confidences.append(confidence)\n",
        "        avg_confidence = np.mean(token_confidences) if token_confidences else 0\n",
        "\n",
        "        # Decode the response and clean it up\n",
        "        decoded_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        output_text = decoded_output.strip()\n",
        "\n",
        "        # Calculate all metrics\n",
        "        anls_score = calculate_anls(output_text, ground_truth_answers)\n",
        "        extractive_match = calculate_extractive_match(output_text, ground_truth_answers)\n",
        "        strict_match = calculate_strict_match(output_text, ground_truth_answers)\n",
        "        strict_match_counts[strict_match] += 1\n",
        "        cm_result = calculate_confusion_matrix(output_text, ground_truth_answers)\n",
        "        for key in confusion_matrix_counts:\n",
        "            confusion_matrix_counts[key] += cm_result[key]\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\n[Document: {file_id}]\")\n",
        "        print(f\"Q: {question}\")\n",
        "        print(f\"A: {output_text}\")\n",
        "        if ground_truth_answers:\n",
        "            print(f\"Ground Truth: {ground_truth_answers[0]}\")\n",
        "            print(f\"ANLS Score: {anls_score:.4f}\")\n",
        "            print(f\"Extractive Match: {extractive_match}\")\n",
        "            print(f\"Strict Match: {strict_match}\")\n",
        "\n",
        "        # Store the result\n",
        "        all_data.append({\n",
        "            \"Document\": file_id,\n",
        "            \"question\": question,\n",
        "            \"output\": output_text,\n",
        "            \"confidence\": avg_confidence,\n",
        "            \"ground_truth\": ground_truth_answers,\n",
        "            \"anls_score\": anls_score,\n",
        "            \"extractive_match\": extractive_match,\n",
        "            \"strict_match\": strict_match,\n",
        "            \"confusion_matrix\": cm_result,\n",
        "        })\n",
        "\n",
        "# Save all results to a JSON file\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(all_data, f, indent=2)\n",
        "\n",
        "print(f\"\\nProcessing complete. All results saved to {output_path}\")"
      ],
      "metadata": {
        "id": "IzI1_W5Bycvb"
      },
      "id": "IzI1_W5Bycvb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if all_data:\n",
        "    # Calculate average ANLS\n",
        "    average_anls = np.mean([item['anls_score'] for item in all_data])\n",
        "\n",
        "    # Calculate average Extractive Match\n",
        "    average_extractive_match = np.mean([item['extractive_match'] for item in all_data])\n",
        "\n",
        "    # Calculate Possible and Actual from strict match counts\n",
        "    possible_count = strict_match_counts[\"Correct\"] + strict_match_counts[\"Incorrect\"] + strict_match_counts[\"Missing\"]\n",
        "    actual_count = strict_match_counts[\"Correct\"] + strict_match_counts[\"Incorrect\"] + strict_match_counts[\"Spurious\"]\n",
        "\n",
        "    print(f\"\\n--- Overall Metrics ---\")\n",
        "    print(f\"Total Questions Processed: {len(all_data)}\")\n",
        "    print(f\"Average ANLS: {average_anls:.4f}\")\n",
        "    print(f\"Average Extractive Match: {average_extractive_match:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Strict Match Breakdown ---\")\n",
        "    for category, count in strict_match_counts.items():\n",
        "        print(f\"{category}: {count}\")\n",
        "    print(f\"\\nPossible: {possible_count}\")\n",
        "    print(f\"Actual: {actual_count}\")\n",
        "\n",
        "    print(\"\\n--- Confusion Matrix (Extractive) ---\")\n",
        "    print(f\"True Positives (TP):  {confusion_matrix_counts['tp']}\")\n",
        "    print(f\"False Positives (FP): {confusion_matrix_counts['fp']}\")\n",
        "    print(f\"True Negatives (TN):  {confusion_matrix_counts['tn']}\")\n",
        "    print(f\"False Negatives (FN): {confusion_matrix_counts['fn']}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo data was processed to calculate average scores.\")"
      ],
      "metadata": {
        "id": "dByD8ykdyflc"
      },
      "id": "dByD8ykdyflc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}