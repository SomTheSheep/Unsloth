{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomTheSheep/Unsloth/blob/master/nb/Qwen2.5_VL_(7B)-Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q9jQA2ravko"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlbeDvvaavkq"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as16UF3Qavkr"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oXanYH2avkr"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5Kmw5Ee0avkr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xLDGk41C7IF"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "9ec1736e-7eac-41d5-844b-2c2c04d00a56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.7.8: Fast Llava patching. Transformers: 4.53.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters.\n",
        "\n",
        "**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True, # False if not finetuning language layers\n",
        "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import TextStreamer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected folder structure\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "pLR3q5BiKkwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up root directories\n",
        "root_dir = \"/DocVQA\"\n",
        "img_dir = os.path.join(root_dir, \"Images\", \"Test\")\n",
        "qa_dir = os.path.join(root_dir, \"QA\", \"Test\")\n",
        "\n",
        "# Output JSON file for results\n",
        "output_path = \"/content/prompt_outputs.json\"\n"
      ],
      "metadata": {
        "id": "kD8MlE1q8ENh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your model and tokenizer initialization\n",
        "# Assumes `model` and `tokenizer` are already loaded elsewhere (or load them here)\n",
        "FastVisionModel.for_inference(model)\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n"
      ],
      "metadata": {
        "id": "9neiodrO8I8w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfcSGwIb6p_R",
        "outputId": "a90d92f1-afad-4be6-8022-9fb837e23453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The routing number on the form is 07201001.</s>\n",
            "Material Approval - Retail Promotion</s>\n",
            "The approval deadline given in the page is January 2, 2001.</s>\n",
            "The deadline for Scientific Abstract Submission for ACOG - 51st Annual Clinical Meeting is December 15, 2020.</s>\n",
            "The deadline for Scientific Abstract Submission for ACOG - 52nd Annual Clinical Meeting is December 15, 2009.</s>\n",
            "The deadline for Scientific Abstract Submission for ARHP - 40th Annual Meeting is December 15, 2009.</s>\n",
            "The deadline for Scientific Abstract Submission for ARHP - 41st Annual Meeting is December 15, 2009.</s>\n",
            "The deadline for scientific abstract submission for the 59th Annual Meeting of the American Society for Reproductive Medicine (ASRM) is January 15, 2010.</s>\n",
            "The deadline for scientific abstract submission for the 60th Annual Meeting of the American Society for Reproductive Medicine (ASRM) is January 15, 2015.</s>\n",
            "The deadline for Scientific Abstract Submission for the 11th Congress on Women's Health is December 15, 2009.</s>\n",
            "The deadline for Scientific Abstract Submission for FIGO - 17th World Congress is December 15, 2009.</s>\n",
            "Winston Racing Nation is a program or organization that focuses on racing and automotive interests. It is likely associated with the Winston brand, which has been involved in various racing events and promotions in the past. The program may offer exclusive benefits, such as discounts or special offers, to its members.</s>\n",
            "The racing-lifestyle information will include details about the Winston Racing Nation program, which is a partnership between Winston and the racing industry. This information will likely cover aspects such as the program's goals, benefits, and requirements for participation. It may also include details about the racing-related events, promotions, and discounts available to program members. Additionally, the information may provide tips on how to maximize the benefits of the program and how to participate in the various racing-related activities and events.</s>\n",
            "The renovation expenses included in the budget for Helen Keller International, Incorporated are $17,000.</s>\n",
            "The total income in the approved budget of Helen Keller International, Incorporated, is $1,325,000.</s>\n",
            "The total expenses in the approved budget of Helen Keller International, Incorporated, amount to $1,358,000.</s>\n",
            "The ALCOHOLIC BEVERAGE MEDICAL RESEARCH FOUNDATION is awarding research grants to Massachusetts General Hospital.</s>\n",
            "The ALCOHOLIC BEVERAGE MEDICAL RESEARCH FOUNDATION is awarding research grants to various individuals, including Dr. Lawrenc F. Bargas, Dr. Peter L. Carlen, Dr. Ivan Diamond, Dr. Scott Eagon, and Dr. David M. Gleiber.</s>\n",
            "The University of Pittsburgh is receiving the ALCOHOLIC BEVERAGE MEDICAL RESEARCH FOUNDATION's Research Grant Awards.</s>\n",
            "The ALCOHOLIC BEVERAGE MEDICAL RESEARCH FOUNDATION is awarding research grants to the Hospital for Sick Children.</s>\n",
            "The ALCOHOLIC BEVERAGE MEDICAL RESEARCH FOUNDATION is awarding research grants to several individuals, including David A. Greenberg, Scott E. Geller, and Peter L. Carlen. These individuals are receiving the grants for their work at Stanford University.</s>\n",
            "In 1987, Dr. Dora B. Goldstein received a grant payment of $15,000.</s>\n",
            "In 1987, the grant payments to Dr. David M. Goldberg amounted to $20,000.</s>\n",
            "In 1987, Dr. Scott E. Geller received a grant payment of $15,000 from the Alcoholic Beverage Medical Research Foundation.</s>\n",
            "In 1987, Dr. Patricia Eagon received a grant payment of $12,000 from the Alcoholic Beverage Medical Research Foundation.</s>\n",
            "The name of the person given in the document is Mrs. William Derby.</s>\n",
            "The birth date of Darby, William J. is January 1, 1923.</s>\n",
            "The birth place of Darby, William J. is listed as \"Tennessee.\"</s>\n",
            "The Telephone No mentioned in this document is 151-3723.</s>\n",
            "Yes, Darby, William J., is a citizen of the United States of America.</s>\n",
            "The Zip code mentioned in the address is 72977.</s>\n",
            "The 'cust#' given is 22932.</s>\n",
            "In the column 'what is your USUAL BRAND of cigarette?', the word \"Marlboro\" is written.</s>\n",
            "In the column 'What is your second choice brand?', the word \"Marlboro\" is written.</s>\n",
            "E.C. Leary is a Title/Position Manager at R.R. Raynott & Co.</s>\n",
            "The evaluation period is one month.</s>\n",
            "The Human Resources department is involved in this situation.</s>\n",
            "The manager is named E. C. Llery.</s>\n",
            "The evaluation period mentioned in the form is March 31, 1995.</s>\n",
            "The company whose logo is displayed in the image is St. Louis Enamel Company.</s>\n",
            "The page number is 22.</s>\n",
            "The date of the estimate is February 20, 1970.</s>\n",
            "The quantity of KW-4 required is 1200 feet.</s>\n",
            "The total estimated amount is $5,300.</s>\n",
            "The title of the sheet is \"CSFF Run Sheet.\"</s>\n",
            "The PD number is 5460.</s>\n",
            "The number in the CSF field is 5404.</s>\n",
            "The Run Nos refers to the specific numbers assigned to each run in the CSF (Cystic Fibrosis) Run Sheet. These numbers are used to track and record the results of each run, such as the weight of the person running, the net pounds lost, and the net pounds gained. The Run Nos help to keep track of the progress and performance of the individual participating in the Cystic Fibrosis Run Sheet.</s>\n",
            "The Net Pounds Infeed is a value that has been calculated by subtracting the Net Pounds Outfeed from the Net Pounds Infeed. This value represents the net gain or loss of poundage in the given time period.</s>\n",
            "The Net Pounds Out is 1230.</s>\n",
            "The website providing information on Camel Snus is CamelSnus.com.</s>\n",
            "The new product launch in the market is scheduled for May 1, 2014.</s>\n",
            "Camel snus is packaged in a tin, which is a common and convenient way to store and transport the product. The tin is typically made of metal, providing a sturdy and durable container for the snus. The tin is often decorated with the Camel brand's logo and design, making it easily recognizable and appealing to customers.</s>\n",
            "The camel snus is restricted to adult consumers only, as indicated in the image.</s>\n",
            "SCGP stands for Shell Catalyst Gasification Process. It is a process used in the production of chemicals and fuels from natural gas. The process involves the conversion of natural gas into synthetic gas, which is then used to produce various chemicals and fuels. The Shell Catalyst Gasification Process is an advanced technology that helps in maximizing the efficiency and yield of the production process.</s>\n",
            "During the SCGP process, coal is converted into a clean and efficient fuel, which can be used for various applications. The process involves converting coal into a synthetic gas, which can then be used as a clean and efficient fuel for power generation, heating, and other industrial processes. This helps reduce the reliance on traditional fossil fuels and contributes to a cleaner and more sustainable energy system.</s>\n",
            "The fourth advanced coal gasification symposium was held in the Netherlands.</s>\n"
          ]
        }
      ],
      "source": [
        "# Collect all QA files\n",
        "qa_files = sorted(os.listdir(qa_dir))\n",
        "\n",
        "# Load or initialize output\n",
        "if os.path.exists(output_path):\n",
        "    with open(output_path, \"r\") as f:\n",
        "        all_data = json.load(f)\n",
        "else:\n",
        "    all_data = []\n",
        "\n",
        "# Iterate through each QA file\n",
        "for qa_filename in qa_files:\n",
        "    file_id = qa_filename.replace(\".json\", \"\")  # like \"ffdh0224_1\"\n",
        "    qa_file_path = os.path.join(qa_dir, qa_filename)\n",
        "    image_file_path = os.path.join(img_dir, f\"{file_id}.png\")\n",
        "\n",
        "    # Skip if image not found\n",
        "    if not os.path.exists(image_file_path):\n",
        "        print(f\"Image not found for {file_id}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(image_file_path).convert(\"RGB\")\n",
        "\n",
        "    # Load questions\n",
        "    with open(qa_file_path, \"r\") as f:\n",
        "        qa_items = json.load(f)\n",
        "\n",
        "    # Ask each question in the file\n",
        "    for qa in qa_items:\n",
        "        question = qa[\"question\"]\n",
        "\n",
        "        # Prepare prompt\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\", \"text\": question}\n",
        "            ]}\n",
        "        ]\n",
        "        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "        inputs = tokenizer(\n",
        "            image,\n",
        "            input_text,\n",
        "            add_special_tokens=False,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        # Generate response\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            streamer=text_streamer,\n",
        "            max_new_tokens=128,\n",
        "            use_cache=True,\n",
        "            temperature=1.5,\n",
        "            min_p=0.1\n",
        "        )\n",
        "\n",
        "        # Decode and clean\n",
        "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        output_text = output_text.replace(input_text, \"\").strip()\n",
        "\n",
        "        # Store result\n",
        "        all_data.append({\n",
        "            \"Document\": file_id,\n",
        "            \"question\": question,\n",
        "            \"output\": output_text\n",
        "        })\n",
        "\n",
        "# Save all results\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(all_data, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcat4UxA81vr",
        "outputId": "7ec4169a-0693-4f33-cf2b-384a1ebd9ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G W Sugar refers to Great Western Sugar Company, which was a sugar company based in Denver, Colorado. The company was known for producing sugar and other related products. The image shows a receipt from the Great Western Sugar Company, which indicates that the company was involved in the sugar trade and distribution.</s>\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n",
        "\n",
        "We use our new `UnslothVisionDataCollator` which will help in our vision finetuning setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95_Nn-89DhsL",
        "outputId": "e810bb24-fc3a-47d1-b942-0932f037b066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Model does not have a default image size - using 512\n"
          ]
        }
      ],
      "source": [
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "FastVisionModel.for_training(model) # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
        "    train_dataset = converted_dataset,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 30,\n",
        "        # num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",     # For Weights and Biases\n",
        "\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns = False,\n",
        "        dataset_text_field = \"\",\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "        max_seq_length = 2048,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "bc91e205-4012-4395-c174-0960d725b84c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "6.068 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "6afd0011-e8a3-478c-aa56-39ed056f586e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 68,686 | Num Epochs = 1 | Total steps = 30\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 51,521,536/7,000,000,000 (0.74% trained)\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 02:59, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.787200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.326000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.431700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.313500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.053400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.038500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.522700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.004700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.716800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.744900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.611900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.499200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.369000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.361600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.353800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.252400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.183800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.196700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.126000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.190200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.183100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.164300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.130300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.098900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.195100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.081400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.235800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.085400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.131800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "63756a82-c628-490c-a005-abb4e7d41b4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "219.9988 seconds used for training.\n",
            "3.67 minutes used for training.\n",
            "Peak reserved memory = 6.484 GB.\n",
            "Peak reserved memory for training = 0.416 GB.\n",
            "Peak reserved memory % of max memory = 43.986 %.\n",
            "Peak reserved memory for training % of max memory = 2.822 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "cd538aab-c139-4c64-aedf-4b95781d6a3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H ^ { \\prime } = \\beta N \\int d \\lambda \\left\\{ \\frac { 1 } { 2 \\beta ^ { 2 } \\bar { N } ^ { 2 } } \\partial _ { \\lambda } \\zeta ^ { \\dagger } \\partial _ { \\lambda } \\zeta + V ( \\lambda ) \\zeta ^ { \\dagger } \\zeta \\right\\} .<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "FastVisionModel.for_inference(model) # Enable for inference!\n",
        "\n",
        "image = dataset[2][\"image\"]\n",
        "instruction = \"Write the LaTeX representation for this image.\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\"},\n",
        "        {\"type\": \"text\", \"text\": instruction}\n",
        "    ]}\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens = False,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "beb1593d-d312-4410-d3ca-040a51017b47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "2b32c167-b6a0-4e80-8a97-9f40806c088f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\frac { N } { M } \\in \\mathbf { Z } , \\frac { P } { Q } \\in \\mathbf { Z } , P \\in \\mathbf { Z }<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "if False:\n",
        "    from unsloth import FastVisionModel\n",
        "    model, tokenizer = FastVisionModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = True, # Set to False for 16bit LoRA\n",
        "    )\n",
        "    FastVisionModel.for_inference(model) # Enable for inference!\n",
        "\n",
        "image = dataset[0][\"image\"]\n",
        "instruction = \"Write the LaTeX representation for this image.\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\"},\n",
        "        {\"type\": \"text\", \"text\": instruction}\n",
        "    ]}\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens = False,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Select ONLY 1 to save! (Both not needed!)\n",
        "\n",
        "# Save locally to 16bit\n",
        "if False: model.save_pretrained_merged(\"unsloth_finetune\", tokenizer,)\n",
        "\n",
        "# To export and save to your Hugging Face account\n",
        "if False: model.push_to_hub_merged(\"YOUR_USERNAME/unsloth_finetune\", tokenizer, token = \"PUT_HERE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3KclRdVC7IQ"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "unsloth_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}